{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T22:13:38.651071Z",
     "start_time": "2018-06-10T22:13:36.904535Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Dropout, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "np.random.seed(7)\n",
    "bbc_data_dir = \"./data/\"\n",
    "glove_embedding_dir = \"./glove.6B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T22:13:38.659440Z",
     "start_time": "2018-06-10T22:13:38.653054Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(directory=bbc_data_dir):\n",
    "    df = pd.DataFrame()\n",
    "    summ = 0\n",
    "    classes = os.listdir(directory)\n",
    "\n",
    "    print(\"Loading:\")\n",
    "    for class_ in classes:\n",
    "        current_class_directory = \"{}{}/\".format(directory, class_)\n",
    "        print(current_class_directory)\n",
    "\n",
    "        for name in sorted(os.listdir(current_class_directory)):\n",
    "            path = os.path.join(current_class_directory, name)\n",
    "\n",
    "            current_text = open(path, encoding = \"ISO-8859-1\")\n",
    "            summ += 1\n",
    "            df.loc[summ,\"text\"] = current_text.read()\n",
    "            df.loc[summ,\"class\"] = class_\n",
    "\n",
    "                      \n",
    "    df[\"class_meaning\"] = df[\"class\"]      \n",
    "    df[\"class\"].replace({\"business\":0,\n",
    "                         \"entertainment\":1,\n",
    "                         \"politics\":2,\n",
    "                         \"sport\":3,\n",
    "                         \"tech\":4},\n",
    "                        inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### This notebook should run from top to bottom. Just run all cells.\n",
    "- #### Your computer should have all the nltk data downloaded for this to run properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project I worked with the BBC raw dataset (http://mlg.ucd.ie/datasets/bbc.html). A neural network classifier was created to classify a given article/text into one out of the five possible categories (business, entertainment, politics, sport and tech)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T22:13:41.461875Z",
     "start_time": "2018-06-10T22:13:38.661342Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = load_dataset()\n",
    "\n",
    "#shuffle data\n",
    "data = data.sample(frac=1,random_state=25)\n",
    "\n",
    "#split into training and test sets in a stratified way - ensuring similar distribution on training and test time\n",
    "train_df, test_df = train_test_split(data,test_size=0.2,\n",
    "                                     random_state=25,\n",
    "                                     stratify=data[\"class\"])\n",
    "\n",
    "train_df = train_df.copy()\n",
    "test_df = test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T22:13:41.678146Z",
     "start_time": "2018-06-10T22:13:41.465408Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize_document(document):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenizer_ = RegexpTokenizer('[a-zA-Z]+')\n",
    "    \n",
    "    words = []\n",
    "    for sentence in sent_tokenize(document):\n",
    "        tokens = [lemmatizer.lemmatize(t.lower()) for t in tokenizer_.tokenize(sentence)\\\n",
    "                  if t.lower() not in stop_words]\n",
    "        words += tokens\n",
    "     \n",
    "    words_ = str()\n",
    "    for word in words:\n",
    "        words_ = words_ + \" \" + word\n",
    "    return words_\n",
    "\n",
    "class DocTokenizer(BaseEstimator, TransformerMixin):\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        X = pd.Series(X)\n",
    "        return X.apply(tokenize_document).tolist()\n",
    "\n",
    "    \n",
    "class WordsEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, top_words = 20000):\n",
    "        self.top_words = top_words\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        encoder = Tokenizer(self.top_words)\n",
    "        encoder.fit_on_texts(X)\n",
    "        self.encoder_ = encoder\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "\n",
    "        return self.encoder_.texts_to_sequences(X)\n",
    "    \n",
    "    \n",
    "class Padder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_sequence_length = 500):\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return sequence.pad_sequences(np.array(X), maxlen = self.max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T22:13:49.237958Z",
     "start_time": "2018-06-10T22:13:41.680369Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessing_pipeline = Pipeline([\n",
    "    ('tokenizer', DocTokenizer()),\n",
    "    ('encoder', WordsEncoder()),\n",
    "    ('padder', Padder())])\n",
    "\n",
    "preprocessing_pipeline.fit(train_df.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T22:14:17.957842Z",
     "start_time": "2018-06-10T22:13:49.240125Z"
    }
   },
   "outputs": [],
   "source": [
    "#load glove embeddings and compute Embedding Matrix\n",
    "\n",
    "glove_embeddings = {}\n",
    "f = open(glove_embedding_dir)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    glove_embeddings[word] = coefs\n",
    "f.close()\n",
    "\n",
    "word_index = preprocessing_pipeline.named_steps['encoder'].encoder_.word_index\n",
    "embedding_dim = len(glove_embeddings['yes']) #=300\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T22:14:18.579154Z",
     "start_time": "2018-06-10T22:14:17.960127Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(np.shape(embedding_matrix)[0],\n",
    "                    np.shape(embedding_matrix)[1],\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(70, activation='relu'))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T14:59:45.027349Z",
     "start_time": "2018-06-10T14:59:39.785381Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = preprocessing_pipeline.transform(train_df.text)\n",
    "y_train = to_categorical(train_df[\"class\"])\n",
    "\n",
    "X_test = preprocessing_pipeline.transform(test_df.text)\n",
    "y_test = to_categorical(test_df[\"class\"])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          epochs=20, \n",
    "          batch_size=128, \n",
    "          validation_split=0.25, \n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T21:38:08.047592Z",
     "start_time": "2018-06-10T21:38:07.795273Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T22:28:45.787630Z",
     "start_time": "2018-06-10T22:28:42.517804Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"model\")\n",
    "joblib.dump(preprocessing_pipeline, \"preprocessing_pipeline.pkl\") \n",
    "\n",
    "loaded_model = load_model(\"model\")\n",
    "loaded_preprocessing_pipeline = joblib.load('preprocessing_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T22:29:03.436536Z",
     "start_time": "2018-06-10T22:29:03.198848Z"
    }
   },
   "outputs": [],
   "source": [
    "categories= {0:\"business\",\n",
    "             1:\"entertainment\",\n",
    "             2:\"politics\",\n",
    "             3:\"sport\",\n",
    "             4:\"tech\"}\n",
    "\n",
    "politics_example = \"But there was disagreement over the rights of future family members - meaning children born in the future to EU citizens in the UK - and the exports of certain social benefits. The EU wants rights currently enjoyed by EU citizens in the UK - access to healthcare, welfare, education - to apply to children and family members, whether they currently live in the UK or not, and to continue in perpetuity, after the death or divorce of the rights-holder. The UK wants to give all EU nationals living in the UK the same rights as British citizens once they have been resident in the country for five years, as long as they arrived before a specified cut-off date, probably 29 March 2017, when Article 50 was triggered. After this date, they could continue to build up their five years' entitlement if necessary. The EU position also requires that citizens live in a host country for five years before acquiring permanent residency rights. In addition, EU nationals who get married after March 2019 would lose the right to bring family members to the UK, unless they pass an income test, like non-EU migrants. They could also risk losing their right to return to Britain if they leave for more than two years. David Davis said the UK had published its approach to citizens rights since the first round of negotiations, which he described as both a fair and serious offer and had now published a joint paper setting out areas of agreement, and issues for further talks. He said sticking points in the talks included the rights of employees of EU-based companies to work for extended periods in other countries, such as the UK. British officials highlighted that British expats would lose the rights to vote and stand in local elections under the EU plan - while the UK position is to protect the rights of citizens to vote and stand in elections in their host country.\"\n",
    "tech_example = \"Kodi is a free, legal media player for computers but software add-ons that in some cases make it possible to download pirated content. The Complete Guide to Kodi magazine instructs readers on how to download such add-ons. Dennis Publishing has not yet responded to a BBC request for comment. The magazine is available at a number of retailers, including WH Smith, Waterstones and Amazon and was spotted on sale by cyber-security researcher Kevin Beaumont. It repeatedly warns readers of the dangers of accessing pirated content online, but one article lists a series of software packages alongside screenshots promoting free TV, popular albums and world sport. Check before you stream and use them at your own risk, the guide says, before adding that readers to stay on the right side of the law. A spokesman for Fact said the body was working with the City of London Police's Intellectual Property Crime Unit (Pipcu) as it made enquiries. We are fully aware of this magazine and have already been in communication with Dennis Publishing regarding our concerns that it signposts consumers to copyright infringing add-ons, said Kieron Sharp, chief executive of Fact. it is concerning that the magazine's content provides information to consumers on add-ons that would potentially allow criminality to take place, he added. In April, the European Court of Justice (ECJ) ruled that selling devices pre-configured with add-ons allowing access to pirated content is illegal, and that streaming such content was also against the law. Two of the add-ons listed in the article are on a banned list maintained by the Kodi developers. We don't support piracy add-ons and so we don't like the idea of someone selling a magazine encouraging people to use them, said Nate Bentzen, Kodi's community and project manager. I am a bit surprised anyone is still selling a magazine like this physically, given all the lawsuits and the recent EU court decision, he added. WHSmith declined to comment but the BBC understands that the newsagent has no plans to stop sales of the magazine. In February, it was reported that five people had been arrested and accused of selling set-top boxes with modified versions of Kodi allowing them to stream subscription football matches, TV channels and films for free.\"\n",
    "entertainment_example = \"For a generation growing up in the early 2000s, it would have been hard not to find someone who didn't own a copy of the band's debut album Hybrid Theory. It's sold more than 30 million copies worldwide and remains one of the biggest selling albums released since the start of the millennium. Linkin Park's successful trick was to fuse elements of metal and rock with rap and hip-hop to shape the nu-metal genre on songs such as Crawling, In The End and Numb. Arguably their biggest asset was Chester's powerhouse voice. He had a huge, raspy vocal which suited their stadium-filling, singalong anthems. Whilst his vocal persona could be described as angry and harsh, in person he was warm, articulate and funny. The band's most recent album, One More Light, saw a different direction as they worked with prolific pop songwriters Julia Michaels and Justin Tranter - and collaborated with UK grime artist Stormzy.\"\n",
    "sports_example = \"Sherida Spitse scored the only goal from the penalty spot after Danielle van de Donk was tripped in the box. Arsenal's Sari van Veenendaal made a superb save to deny Denmark's Pernille Harder, while Nadia Nadim headed straight at the goalkeeper late on. Two-time champions Norway are on the brink of elimination after a surprise 2-0 defeat by Belgium. Elke van Gorp came from an offside position to poke in from Ingrid Hjelmseth's parry and give debutants Belgium the lead on 59 minutes. They doubled their lead through Janice Cayman's free header after the Norwegian defenders failed to clear. Norway's best chance fell to Caroline Graham Hansen, but the forward volleyed over the crossbar from close range. They were on top in the first half as Andrine Hegerberg had a flicked effort pushed away, but her sister Ada - the BBC Women's Footballer of the Year - struggled to make an impact. Martin Sjogren's Norway side - runners-up in 2013 - lost to hosts Netherlands in their first game and must now beat Denmark on Monday to have any chance of progressing. Meanwhile, a point for Netherlands in their final group game against Belgium will seal progression to the knockout stages.\"\n",
    "business_example = \"Chinese media have mocked US President Donald Trump over plans to impose 25% tariffs on $50bn worth of Chinese goods, saying wise men build bridges but fools build walls. Mr Trump announced the tariffs on Friday, accusing Beijing of intellectual copyright theft. China retaliated, saying it would impose an additional 25% tariff on 659 US goods worth $50bn. Stock markets fell after the announcements amid fear of a trade war. The US had earlier warned that it will impose even more tariffs should China retaliate. Mr Trump said the tariffs were essential to preventing further unfair transfers of American technology and intellectual property to China, which will protect American jobs. The Chinese product lines that have been hit range from aircraft tyres to turbines and commercial dishwashers. What is a trade war and why should I worry China vows fast response to US tariffs G7 summit ends in disarray over tariffs US tariffs a dangerous game, says EU State-controlled media made a concerted attack on the new US measures. Following the path of expanding and opening up is China's best response to the trade dispute between China and the United States, and is also the responsibility that major countries should have to the world, said an editorial in Xinhua news agency. The wise man builds bridges, the fool builds walls, it commented. Social media users were quick to make light of the comment, with many making reference to the Great Wall of China.\"\n",
    "\n",
    "processed_text = loaded_preprocessing_pipeline.transform(entertainment_example)\n",
    "probas = loaded_model.predict_proba(processed_text, verbose=0)*100\n",
    "\n",
    "print('business: {0:.2f}%'.format(probas[0][0]))\n",
    "print('entertainment: {0:.2f}%'.format(probas[0][1]))\n",
    "print('politics: {0:.2f}%'.format(probas[0][2]))\n",
    "print('sport: {0:.2f}%'.format(probas[0][3]))\n",
    "print('tech: {0:.2f}%'.format(probas[0][4]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
